{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Requiured Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)# To see all the columns of a dataframe\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce the memory usage of various Dataframes\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "       \n",
    "        1. Iterate over every column\n",
    "        2. Determine if the column is numeric\n",
    "        3. Determine if the column can be represented by an integer\n",
    "        4. Find the min and the max value\n",
    "        5. Determine and apply the smallest datatype that can fit the range of values\n",
    "\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 181.24 MB\n",
      "Memory usage after optimization is: 38.27 MB\n",
      "Decreased by 78.9%\n"
     ]
    }
   ],
   "source": [
    "# Loading reduced feature training set\n",
    "X_train = import_data('X_train_final.csv')\n",
    "y_train = pd.read_csv('y_train.final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 60.41 MB\n",
      "Memory usage after optimization is: 12.76 MB\n",
      "Decreased by 78.9%\n"
     ]
    }
   ],
   "source": [
    "# Loading reduced feature test set\n",
    "X_test = import_data('X_test_final.csv')\n",
    "y_test = pd.read_csv('y_test.final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_5: XGboost Classifier with Tuned Hyperparameters using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Sklearn's roc_auc_score module\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Libraries\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Stratified K fold object\n",
    "cv_strat = StratifiedKFold(5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing  hyperparamater tuning optimizer optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Xgboost Classifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the ratio of -ve to +ve classes. Note here class_weight is a series and should be converted to float value\n",
    "# using float() to be used as a sclar inside xgboost. \n",
    "class_weight = float((y_train==0).sum()/(y_train==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the appropriate objective function for the XGboost classifier\n",
    "\n",
    "def objective_wrappper_xgb(X_tr, y_tr, cls=None, cv_strat=None, cl_w=None):\n",
    "    '''\n",
    "    Optimizes classifier's cls (Xgboost here) parameters on the given training set X_tr,y_tr\n",
    "    using cross-validation cv_strat & Class weights cl_w objects.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "        #'scale_pos_weight': trial.suggest_categorical('scale_pos_weight',[class_weight,9,10]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 400),\n",
    "        'gamma': trial.suggest_int('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_uniform('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda', 0, 10),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 1, 10),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 75),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n",
    "        }\n",
    "        cls.set_params(**params)#Initializing the model with the parameters \n",
    "    \n",
    "        return np.mean(cross_val_score(cls, X_tr, y_tr, cv=cv_strat, n_jobs=5, scoring='roc_auc'))  \n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the evaluation function for study's best parameters\n",
    "def study_best_score_params(X_tr, y_tr, cls, obj_func, cv_strat, cl_w=None, n_trials=100):\n",
    "    ''' Computes the best hyper parameters of the classsifier and returns \n",
    "    Optuna's study's best score & clasifier parameters'''\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(obj_func(X_tr, y_tr, cls, cv_strat, cl_w), n_trials)\n",
    "    best_score = study.best_value\n",
    "    best_params = study.best_params\n",
    "    return (best_score,best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the xgboost classifier  \n",
    "xgb_s = xgb.XGBClassifier(random_state=42,n_jobs=5,objective='binary:logistic',scale_pos_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the best hyperparameters for the  XGboost Classifier using Reduced feature Training Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-29 19:52:27,775]\u001b[0m A new study created in memory with name: no-name-89669194-2e29-4139-9f9f-fba2759a1009\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 19:55:04,573]\u001b[0m Trial 0 finished with value: 0.7399930764722756 and parameters: {'n_estimators': 60, 'gamma': 4, 'reg_alpha': 6.461297110356901, 'reg_lambda': 3.0651989087218023, 'max_delta_step': 10, 'max_depth': 21, 'colsample_bytree': 0.8139076755003545, 'learning_rate': 0.009908592475110082}. Best is trial 0 with value: 0.7399930764722756.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 20:00:51,137]\u001b[0m Trial 1 finished with value: 0.7427592646621521 and parameters: {'n_estimators': 78, 'gamma': 0, 'reg_alpha': 2.936107049035633, 'reg_lambda': 7.615633231837846, 'max_delta_step': 4, 'max_depth': 48, 'colsample_bytree': 0.9706296338541937, 'learning_rate': 0.03717515418317997}. Best is trial 1 with value: 0.7427592646621521.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 20:25:00,319]\u001b[0m Trial 2 finished with value: 0.7132375519974194 and parameters: {'n_estimators': 318, 'gamma': 0, 'reg_alpha': 3.447065752313102, 'reg_lambda': 3.840096719238769, 'max_delta_step': 5, 'max_depth': 57, 'colsample_bytree': 0.9664084292748115, 'learning_rate': 0.0010689791819209998}. Best is trial 1 with value: 0.7427592646621521.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 20:44:17,326]\u001b[0m Trial 3 finished with value: 0.7544760036217839 and parameters: {'n_estimators': 264, 'gamma': 5, 'reg_alpha': 7.561291685789646, 'reg_lambda': 1.1386993331742723, 'max_delta_step': 6, 'max_depth': 67, 'colsample_bytree': 0.8603172539930053, 'learning_rate': 0.05392608286962822}. Best is trial 3 with value: 0.7544760036217839.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 20:50:18,680]\u001b[0m Trial 4 finished with value: 0.7456823618815168 and parameters: {'n_estimators': 109, 'gamma': 5, 'reg_alpha': 4.578664377818028, 'reg_lambda': 2.6494813231135197, 'max_delta_step': 10, 'max_depth': 57, 'colsample_bytree': 0.702625443091168, 'learning_rate': 0.004051875403935378}. Best is trial 3 with value: 0.7544760036217839.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 20:51:41,565]\u001b[0m Trial 5 finished with value: 0.696963081138633 and parameters: {'n_estimators': 18, 'gamma': 3, 'reg_alpha': 2.2142077661495465, 'reg_lambda': 3.981892709368627, 'max_delta_step': 6, 'max_depth': 62, 'colsample_bytree': 0.9625500960468989, 'learning_rate': 0.004275221499879809}. Best is trial 3 with value: 0.7544760036217839.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 21:08:08,376]\u001b[0m Trial 6 finished with value: 0.7557830350327148 and parameters: {'n_estimators': 321, 'gamma': 1, 'reg_alpha': 0.695111752177342, 'reg_lambda': 6.075320074267467, 'max_delta_step': 2, 'max_depth': 61, 'colsample_bytree': 0.6685034021510392, 'learning_rate': 0.011216585119306087}. Best is trial 6 with value: 0.7557830350327148.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 21:17:48,813]\u001b[0m Trial 7 finished with value: 0.7377129926430683 and parameters: {'n_estimators': 149, 'gamma': 2, 'reg_alpha': 4.2552072968031105, 'reg_lambda': 4.8627968793062895, 'max_delta_step': 4, 'max_depth': 48, 'colsample_bytree': 0.8260692751926307, 'learning_rate': 0.001461472530773722}. Best is trial 6 with value: 0.7557830350327148.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 21:18:22,060]\u001b[0m Trial 8 finished with value: 0.751485040134196 and parameters: {'n_estimators': 81, 'gamma': 4, 'reg_alpha': 6.023336169153079, 'reg_lambda': 9.624366745856438, 'max_delta_step': 9, 'max_depth': 6, 'colsample_bytree': 0.6774052209111503, 'learning_rate': 0.033399591798462476}. Best is trial 6 with value: 0.7557830350327148.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 21:34:21,101]\u001b[0m Trial 9 finished with value: 0.7458607403983789 and parameters: {'n_estimators': 341, 'gamma': 4, 'reg_alpha': 4.9666509549644156, 'reg_lambda': 9.239954929581248, 'max_delta_step': 5, 'max_depth': 24, 'colsample_bytree': 0.7749071058246944, 'learning_rate': 0.001948053622978066}. Best is trial 6 with value: 0.7557830350327148.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 21:52:33,678]\u001b[0m Trial 10 finished with value: 0.7591274741255042 and parameters: {'n_estimators': 389, 'gamma': 1, 'reg_alpha': 0.15158769271517225, 'reg_lambda': 6.969994643163497, 'max_delta_step': 1, 'max_depth': 72, 'colsample_bytree': 0.6053239134623578, 'learning_rate': 0.015250833829331975}. Best is trial 10 with value: 0.7591274741255042.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 22:10:47,921]\u001b[0m Trial 11 finished with value: 0.7576427123208085 and parameters: {'n_estimators': 390, 'gamma': 1, 'reg_alpha': 0.32338219664064555, 'reg_lambda': 6.750464593816613, 'max_delta_step': 1, 'max_depth': 75, 'colsample_bytree': 0.6028397512098703, 'learning_rate': 0.011888728399931976}. Best is trial 10 with value: 0.7591274741255042.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 22:28:48,614]\u001b[0m Trial 12 finished with value: 0.7586560119473362 and parameters: {'n_estimators': 386, 'gamma': 1, 'reg_alpha': 0.48547767863705893, 'reg_lambda': 7.0822403007646555, 'max_delta_step': 1, 'max_depth': 75, 'colsample_bytree': 0.6022681049952064, 'learning_rate': 0.015903583768034142}. Best is trial 10 with value: 0.7591274741255042.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 22:47:53,526]\u001b[0m Trial 13 finished with value: 0.7601220750759048 and parameters: {'n_estimators': 400, 'gamma': 1, 'reg_alpha': 1.4085416334835972, 'reg_lambda': 8.290929525027488, 'max_delta_step': 1, 'max_depth': 74, 'colsample_bytree': 0.6112524657997388, 'learning_rate': 0.02324974668768498}. Best is trial 13 with value: 0.7601220750759048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 23:00:43,118]\u001b[0m Trial 14 finished with value: 0.751501038398531 and parameters: {'n_estimators': 254, 'gamma': 2, 'reg_alpha': 1.701908911657839, 'reg_lambda': 8.44926044025516, 'max_delta_step': 2, 'max_depth': 75, 'colsample_bytree': 0.6067127010767234, 'learning_rate': 0.08820242882437217}. Best is trial 13 with value: 0.7601220750759048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 23:21:13,308]\u001b[0m Trial 15 finished with value: 0.756669937551177 and parameters: {'n_estimators': 400, 'gamma': 0, 'reg_alpha': 9.566868378972137, 'reg_lambda': 5.62115278064763, 'max_delta_step': 3, 'max_depth': 33, 'colsample_bytree': 0.7354814606682841, 'learning_rate': 0.021338301355227584}. Best is trial 13 with value: 0.7601220750759048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 23:31:48,040]\u001b[0m Trial 16 finished with value: 0.751617083648636 and parameters: {'n_estimators': 209, 'gamma': 1, 'reg_alpha': 1.5577814103705425, 'reg_lambda': 8.205140554085894, 'max_delta_step': 8, 'max_depth': 69, 'colsample_bytree': 0.64357682635227, 'learning_rate': 0.005829988954599256}. Best is trial 13 with value: 0.7601220750759048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-29 23:49:42,053]\u001b[0m Trial 17 finished with value: 0.7604269855174032 and parameters: {'n_estimators': 351, 'gamma': 2, 'reg_alpha': 0.20215695731706032, 'reg_lambda': 9.876625377718598, 'max_delta_step': 1, 'max_depth': 46, 'colsample_bytree': 0.6402494837190663, 'learning_rate': 0.0270630848171949}. Best is trial 17 with value: 0.7604269855174032.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:10:49,451]\u001b[0m Trial 18 finished with value: 0.7515202138440739 and parameters: {'n_estimators': 347, 'gamma': 3, 'reg_alpha': 2.820681257277131, 'reg_lambda': 9.991846803000222, 'max_delta_step': 2, 'max_depth': 44, 'colsample_bytree': 0.7385318073798498, 'learning_rate': 0.09867857217966654}. Best is trial 17 with value: 0.7604269855174032.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:12:06,998]\u001b[0m Trial 19 finished with value: 0.7616536781422276 and parameters: {'n_estimators': 259, 'gamma': 2, 'reg_alpha': 1.1793635199547952, 'reg_lambda': 8.944938894393932, 'max_delta_step': 3, 'max_depth': 5, 'colsample_bytree': 0.6478438160394054, 'learning_rate': 0.028028587183674687}. Best is trial 19 with value: 0.7616536781422276.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:13:33,032]\u001b[0m Trial 20 finished with value: 0.7653360616080642 and parameters: {'n_estimators': 236, 'gamma': 2, 'reg_alpha': 9.665486607047548, 'reg_lambda': 9.147237022712792, 'max_delta_step': 3, 'max_depth': 6, 'colsample_bytree': 0.6499073257076501, 'learning_rate': 0.05820583531821543}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:14:36,875]\u001b[0m Trial 21 finished with value: 0.7649901968373112 and parameters: {'n_estimators': 274, 'gamma': 2, 'reg_alpha': 9.48619751107778, 'reg_lambda': 9.23631685544649, 'max_delta_step': 3, 'max_depth': 4, 'colsample_bytree': 0.6402456248619234, 'learning_rate': 0.05624549943457186}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-30 00:15:23,014]\u001b[0m Trial 22 finished with value: 0.7613284330679144 and parameters: {'n_estimators': 238, 'gamma': 3, 'reg_alpha': 9.899992443768694, 'reg_lambda': 8.952592161647805, 'max_delta_step': 3, 'max_depth': 3, 'colsample_bytree': 0.7048275001505966, 'learning_rate': 0.05718306911861545}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:19:19,561]\u001b[0m Trial 23 finished with value: 0.7547546814816826 and parameters: {'n_estimators': 286, 'gamma': 2, 'reg_alpha': 8.897534428056064, 'reg_lambda': 9.096711364349284, 'max_delta_step': 3, 'max_depth': 11, 'colsample_bytree': 0.6478299154867042, 'learning_rate': 0.05432015025761056}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:22:47,998]\u001b[0m Trial 24 finished with value: 0.7522154472481712 and parameters: {'n_estimators': 179, 'gamma': 2, 'reg_alpha': 8.058538512617575, 'reg_lambda': 7.958900607838448, 'max_delta_step': 4, 'max_depth': 13, 'colsample_bytree': 0.7035392170956127, 'learning_rate': 0.07188160737397135}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:29:59,549]\u001b[0m Trial 25 finished with value: 0.7573068102339999 and parameters: {'n_estimators': 215, 'gamma': 3, 'reg_alpha': 8.813322703151423, 'reg_lambda': 9.7405672967031, 'max_delta_step': 7, 'max_depth': 18, 'colsample_bytree': 0.7640682294797791, 'learning_rate': 0.0427273441919897}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:32:47,936]\u001b[0m Trial 26 finished with value: 0.7598472112443948 and parameters: {'n_estimators': 291, 'gamma': 2, 'reg_alpha': 6.965276812877887, 'reg_lambda': 7.625587905210404, 'max_delta_step': 3, 'max_depth': 7, 'colsample_bytree': 0.9219680960464546, 'learning_rate': 0.07771770606132732}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:33:13,592]\u001b[0m Trial 27 finished with value: 0.7419312317220647 and parameters: {'n_estimators': 190, 'gamma': 3, 'reg_alpha': 9.174498807990764, 'reg_lambda': 9.995149992785032, 'max_delta_step': 4, 'max_depth': 2, 'colsample_bytree': 0.6627820615638201, 'learning_rate': 0.030949949784623426}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:39:46,686]\u001b[0m Trial 28 finished with value: 0.7550940204905491 and parameters: {'n_estimators': 157, 'gamma': 2, 'reg_alpha': 8.25078591002028, 'reg_lambda': 8.873935585851768, 'max_delta_step': 2, 'max_depth': 27, 'colsample_bytree': 0.6327843233455842, 'learning_rate': 0.044715214425861224}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:47:24,898]\u001b[0m Trial 29 finished with value: 0.7517963566856883 and parameters: {'n_estimators': 247, 'gamma': 3, 'reg_alpha': 5.976872140784099, 'reg_lambda': 0.47956352858166174, 'max_delta_step': 5, 'max_depth': 17, 'colsample_bytree': 0.6936915301740473, 'learning_rate': 0.007533513808137669}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:50:23,446]\u001b[0m Trial 30 finished with value: 0.7603275221255803 and parameters: {'n_estimators': 277, 'gamma': 2, 'reg_alpha': 6.7030099201976725, 'reg_lambda': 6.18707303090127, 'max_delta_step': 3, 'max_depth': 9, 'colsample_bytree': 0.6337570362788552, 'learning_rate': 0.017922626075385324}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:51:11,994]\u001b[0m Trial 31 finished with value: 0.7619926880963858 and parameters: {'n_estimators': 240, 'gamma': 3, 'reg_alpha': 9.919504055831599, 'reg_lambda': 8.828604374995026, 'max_delta_step': 3, 'max_depth': 3, 'colsample_bytree': 0.7381208705330278, 'learning_rate': 0.06054427146857679}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:51:57,265]\u001b[0m Trial 32 finished with value: 0.7621880344186307 and parameters: {'n_estimators': 225, 'gamma': 4, 'reg_alpha': 9.821925724496971, 'reg_lambda': 8.494857233810082, 'max_delta_step': 4, 'max_depth': 3, 'colsample_bytree': 0.7461062061041204, 'learning_rate': 0.06610142512713675}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:58:16,526]\u001b[0m Trial 33 finished with value: 0.7536750208440106 and parameters: {'n_estimators': 233, 'gamma': 4, 'reg_alpha': 9.263145009441814, 'reg_lambda': 7.519729864295588, 'max_delta_step': 4, 'max_depth': 15, 'colsample_bytree': 0.8234036246016073, 'learning_rate': 0.0670297883931754}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 00:58:59,635]\u001b[0m Trial 34 finished with value: 0.7623788659476622 and parameters: {'n_estimators': 305, 'gamma': 4, 'reg_alpha': 9.87958744920816, 'reg_lambda': 8.55912876846733, 'max_delta_step': 5, 'max_depth': 2, 'colsample_bytree': 0.7291013024006138, 'learning_rate': 0.0990567607802031}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:03:58,616]\u001b[0m Trial 35 finished with value: 0.75651351244921 and parameters: {'n_estimators': 306, 'gamma': 5, 'reg_alpha': 9.983417705466438, 'reg_lambda': 7.73433683518171, 'max_delta_step': 5, 'max_depth': 11, 'colsample_bytree': 0.7788302539600254, 'learning_rate': 0.04325308097740202}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:17:24,368]\u001b[0m Trial 36 finished with value: 0.7520091198123224 and parameters: {'n_estimators': 305, 'gamma': 4, 'reg_alpha': 8.346484860898753, 'reg_lambda': 9.376904395242052, 'max_delta_step': 6, 'max_depth': 22, 'colsample_bytree': 0.8602961535854126, 'learning_rate': 0.08976518292026875}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:17:55,436]\u001b[0m Trial 37 finished with value: 0.7591405106527314 and parameters: {'n_estimators': 219, 'gamma': 4, 'reg_alpha': 7.460401615140269, 'reg_lambda': 1.9299074278487396, 'max_delta_step': 5, 'max_depth': 2, 'colsample_bytree': 0.7218534985460948, 'learning_rate': 0.0907634430040012}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:19:47,433]\u001b[0m Trial 38 finished with value: 0.7567895741544475 and parameters: {'n_estimators': 188, 'gamma': 5, 'reg_alpha': 8.6110195726303, 'reg_lambda': 8.39291414736012, 'max_delta_step': 7, 'max_depth': 8, 'colsample_bytree': 0.7626804545426845, 'learning_rate': 0.09898395292765047}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:34:27,279]\u001b[0m Trial 39 finished with value: 0.7581826021649183 and parameters: {'n_estimators': 269, 'gamma': 4, 'reg_alpha': 7.678007674242613, 'reg_lambda': 4.735108245927663, 'max_delta_step': 4, 'max_depth': 30, 'colsample_bytree': 0.8102729401161842, 'learning_rate': 0.039514557591030265}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:43:49,952]\u001b[0m Trial 40 finished with value: 0.7578424305751861 and parameters: {'n_estimators': 328, 'gamma': 5, 'reg_alpha': 9.53696245861002, 'reg_lambda': 7.368827694220708, 'max_delta_step': 6, 'max_depth': 18, 'colsample_bytree': 0.6754549107059554, 'learning_rate': 0.05232507351903775}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:44:23,203]\u001b[0m Trial 41 finished with value: 0.7564021464520037 and parameters: {'n_estimators': 233, 'gamma': 3, 'reg_alpha': 9.932388843551788, 'reg_lambda': 8.573156011006777, 'max_delta_step': 2, 'max_depth': 2, 'colsample_bytree': 0.7369894630879804, 'learning_rate': 0.06447949234152774}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:45:18,622]\u001b[0m Trial 42 finished with value: 0.7642172152857094 and parameters: {'n_estimators': 169, 'gamma': 3, 'reg_alpha': 9.957583112688416, 'reg_lambda': 6.494209866527345, 'max_delta_step': 4, 'max_depth': 5, 'colsample_bytree': 0.7209574178438277, 'learning_rate': 0.07116024618707474}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:46:06,834]\u001b[0m Trial 43 finished with value: 0.762980572987955 and parameters: {'n_estimators': 119, 'gamma': 4, 'reg_alpha': 9.44697316955564, 'reg_lambda': 6.658573512828465, 'max_delta_step': 4, 'max_depth': 6, 'colsample_bytree': 0.7150275588056015, 'learning_rate': 0.07139206289303121}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:47:39,373]\u001b[0m Trial 44 finished with value: 0.7547522326013179 and parameters: {'n_estimators': 114, 'gamma': 3, 'reg_alpha': 9.092968992761143, 'reg_lambda': 6.218579768681672, 'max_delta_step': 5, 'max_depth': 10, 'colsample_bytree': 0.7140887310658695, 'learning_rate': 0.07752408852716298}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-30 01:50:33,853]\u001b[0m Trial 45 finished with value: 0.7459880495760225 and parameters: {'n_estimators': 124, 'gamma': 4, 'reg_alpha': 9.421698227088745, 'reg_lambda': 5.511374533273388, 'max_delta_step': 4, 'max_depth': 14, 'colsample_bytree': 0.6877849347781635, 'learning_rate': 0.0026112989981588895}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:51:16,297]\u001b[0m Trial 46 finished with value: 0.7538414106811981 and parameters: {'n_estimators': 94, 'gamma': 4, 'reg_alpha': 7.840314418561915, 'reg_lambda': 6.612710993880608, 'max_delta_step': 5, 'max_depth': 6, 'colsample_bytree': 0.7919852239984206, 'learning_rate': 0.03486241525285229}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:51:42,731]\u001b[0m Trial 47 finished with value: 0.7530426832305496 and parameters: {'n_estimators': 54, 'gamma': 5, 'reg_alpha': 8.556811625152534, 'reg_lambda': 4.324513727303082, 'max_delta_step': 4, 'max_depth': 7, 'colsample_bytree': 0.6698789291071952, 'learning_rate': 0.04747851035693326}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:54:46,476]\u001b[0m Trial 48 finished with value: 0.7467473344313953 and parameters: {'n_estimators': 154, 'gamma': 3, 'reg_alpha': 3.8448826107923466, 'reg_lambda': 5.211350443768536, 'max_delta_step': 2, 'max_depth': 13, 'colsample_bytree': 0.7216913581652858, 'learning_rate': 0.07794430392153769}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n",
      "\u001b[32m[I 2020-12-30 01:59:54,048]\u001b[0m Trial 49 finished with value: 0.7455410525457096 and parameters: {'n_estimators': 140, 'gamma': 1, 'reg_alpha': 7.215465369179558, 'reg_lambda': 6.558554338955444, 'max_delta_step': 5, 'max_depth': 20, 'colsample_bytree': 0.692982685141779, 'learning_rate': 0.001034308664921631}. Best is trial 20 with value: 0.7653360616080642.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Extracting the best model parameters and best study score\n",
    "best_study_score,best_study_params = study_best_score_params(X_train.values, y_train.values, xgb_s, objective_wrappper_xgb, cv_strat=cv_strat,\n",
    "                                                       n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best roc_auc_score for the study is:  0.7653360616080642\n"
     ]
    }
   ],
   "source": [
    "print('The best roc_auc_score for the study is: ',best_study_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best study parameters for the classifier are:  {'n_estimators': 236, 'gamma': 2, 'reg_alpha': 9.665486607047548, 'reg_lambda': 9.147237022712792, 'max_delta_step': 3, 'max_depth': 6, 'colsample_bytree': 0.6499073257076501, 'learning_rate': 0.05820583531821543}\n"
     ]
    }
   ],
   "source": [
    "print('The best study parameters for the classifier are: ',best_study_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the best Xgboost model by setting best study parameters.\n",
    "xgb_s = xgb_s.set_params(**best_study_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(colsample_bytree=0.6499073257076501, gamma=2,\n",
       "              learning_rate=0.05820583531821543, max_delta_step=3, max_depth=6,\n",
       "              n_estimators=236, n_jobs=5, random_state=42,\n",
       "              reg_alpha=9.665486607047548, reg_lambda=9.147237022712792,\n",
       "              scale_pos_weight=11.386970299156776)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the best Xgboost model on the whole Reduced feature training set\n",
    "xgb_s.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to calculate the roc_auc score for the feature sets\n",
    "def cal_roc_auc(X, y, cls, f_set, t_set, model_name):\n",
    "    ''' Calculates the roc auc score using the best study parameters \n",
    "        f_set : String: specifies 'full feature', 'Reduced feature'\n",
    "        t_set: String: specifies 'training', 'test'\n",
    "        model_name: String: specifies Name of the model '''\n",
    "        \n",
    "    y_pred = cls.predict_proba(X)\n",
    "    print('The roc_auc_score for the {} {} set using the best {} classifier is '.format(f_set,t_set,model_name),roc_auc_score(y, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score for the Reduced feature training set using the best Xgboost classifier is  0.8401392081413007\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Reduced feature training set roc_auc score using the best study parameters\n",
    "cal_roc_auc(X_train.values, y_train.values, xgb_s, 'Reduced feature', 'training', 'Xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score for the Reduced feature test set using the best Xgboost classifier is  0.7691327155992314\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Reduced feature test set roc_auc score using the best study parameters\n",
    "cal_roc_auc(X_test.values, y_test.values, xgb_s, 'Reduced feature', 'test', 'Xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating R_R ratio for best Xgboost Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the CV scores using sklearn's cross_val_score\n",
    "score_xgb = cross_val_score(xgb_s, X_train.values, y_train.values, cv=cv_strat, n_jobs=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward associated with the best Xgboost Classifier using roc_auc metric is:  0.7653360616080642\n"
     ]
    }
   ],
   "source": [
    "print('The reward associated with the best Xgboost Classifier using roc_auc metric is: ',np.mean(score_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The risk associated with the best Xgboost Classifier using roc_auc metric is:  0.004942475986184466\n"
     ]
    }
   ],
   "source": [
    "print('The risk associated with the best Xgboost Classifier using roc_auc metric is: ',np.std(score_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_R_Ratio_Xgboost = np.mean(score_xgb)/np.std(score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward risk ratio for the best Xgboost Classifier using roc_auc metric is:  154.84871626030798\n"
     ]
    }
   ],
   "source": [
    "print('The reward risk ratio for the best Xgboost Classifier using roc_auc metric is: ',R_R_Ratio_Xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGboost.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the Reduced feature set best Xgboost Classifier \n",
    "import joblib\n",
    "joblib.dump(xgb_s, 'XGboost.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R_R Ratio for the Xgboost classifier using roc_auc metric is:  154.84871626030798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "### 1) The xgboost classifier's test set roc_auc score & R_R ratio are slightly less than those of light Gbm classifier_2. May be more extensive hyperparameter search , might result in better score for xgboost classifier.\n",
    "### 2) The xgboost classifier is clearly overfitting the dataset, which is evident looking at the difference between training set & test set roc_auc scores.\n",
    "### 3) Tuning xgboost requires more computational resources, which may be better done on cloud than on PC & since we are already getting good performance using Light Gbm classifier, we won't be further tuning Xgboost classifier for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R_R Ratio for the best Tree based boosting classifier using roc_auc score is:  167.37634851965672,  corresponding to Tuned more regularized Light Gbm classifier_2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking everything into consideration, such as  Overfitting, test set roc_auc score, R_R ratios & Computational costs, the best tree based boosting classifier is Light Gbm classifier_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
